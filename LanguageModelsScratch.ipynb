{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Language Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Pad each Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MarianaMaroto\\Desktop\\MS in Data Science\\74040 NLP\\Homework 1\n"
     ]
    }
   ],
   "source": [
    "%cd  /Users/MarianaMaroto/Desktop/MS in Data Science/74040 NLP/Homework 1/\n",
    "\n",
    "# open the input file for reading and create a new output file for writing\n",
    "readfile = open('train.txt', 'r',  errors=\"ignore\")\n",
    "writefile = open('train1.txt', 'w')\n",
    "\n",
    "# read each line in the input file\n",
    "for line in readfile:\n",
    "    # remove the trailing carriage return\n",
    "    line = line.strip()\n",
    "    writefile.write('<s>' + ' ' + line + ' ' + '</s>' + '\\n')\n",
    "writefile.close()\n",
    "readfile.close()\n",
    "    \n",
    "# open the input file for reading and create a new output file for writing\n",
    "readfile2 = open('test.txt', 'r',  errors=\"ignore\")\n",
    "writefile2 = open('test1.txt', 'w')\n",
    "\n",
    "# read each line in the input file\n",
    "for line in readfile2:\n",
    "    # remove the trailing carriage return\n",
    "    line = line.strip()\n",
    "    writefile2.write('<s>' + ' ' + line + ' ' + '</s>' + '\\n')\n",
    "writefile2.close()\n",
    "readfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Lowercase all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the input file for reading and create a new output file for writing\n",
    "readfile = open('train1.txt', 'r',  errors=\"ignore\")\n",
    "writefile = open('train2.txt', 'w')\n",
    "\n",
    "# read each line in the input file\n",
    "for line in readfile:\n",
    "    line = line.lower()\n",
    "    writefile.write(line)\n",
    "writefile.close()   \n",
    "readfile.close()\n",
    "\n",
    "# open the input file for reading and create a new output file for writing\n",
    "readfile2 = open('test1.txt', 'r',  errors=\"ignore\")\n",
    "writefile2 = open('test2.txt', 'w')\n",
    "\n",
    "# read each line in the input file\n",
    "for line in readfile2:\n",
    "    line = line.lower()\n",
    "    writefile2.write(line)\n",
    "writefile2.close()\n",
    "readfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.  Replace all words occurring in the training data once with the token unk. Every word in the test data not seen in training should be treated as unk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "text = open(\"train2.txt\", \"r\", errors=\"ignore\")\n",
    "  \n",
    "# Create an empty dictionary\n",
    "d = dict()\n",
    "  \n",
    "# Loop through each line of the file\n",
    "for line in text:\n",
    "    # Split the line into words\n",
    "    words = line.split(\" \")\n",
    "    for word in words:\n",
    "        # Check if the word is already in dictionary\n",
    "        if word in d:\n",
    "            # Increment count of word by 1\n",
    "            d[word] = d[word] + 1\n",
    "        else:\n",
    "            # Add the word to dictionary with count 1\n",
    "            d[word] = 1\n",
    "            \n",
    "d_once = dict((k, v) for k, v in d.items() if v == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the input file for reading and create a new output file for writing\n",
    "readfile = open('train2.txt', 'r',  errors=\"ignore\")\n",
    "writefile = open('train3.txt', 'w')\n",
    "\n",
    "# read each line in the input file\n",
    "for line in readfile:\n",
    "# Split the line into words\n",
    "    words = line.split(\" \")\n",
    "    for word in words:\n",
    "        if word in d_once:\n",
    "            line = line.replace(' ' + word + ' ',' ' + '<unk>'+ ' ')\n",
    "    writefile.write(line)\n",
    "writefile.close()   \n",
    "readfile.close()\n",
    "    \n",
    "# open the input file for reading and create a new output file for writing\n",
    "readfile2 = open('test2.txt', 'r',  errors=\"ignore\")\n",
    "writefile2 = open('test3.txt', 'w')\n",
    "\n",
    "# read each line in the input file\n",
    "for line in readfile2:\n",
    "# Split the line into words\n",
    "    words = line.split(\" \")\n",
    "    for word in words:\n",
    "        if word not in d or word in d_once:\n",
    "            line = line.replace(' ' + word + ' ',' ' + '<unk>'+ ' ')\n",
    "    writefile2.write(line)\n",
    "writefile2.close()   \n",
    "readfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. A unigram maximum likelihood model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "def CountUnigrams(file):\n",
    "    \"\"\"\n",
    "    Count number of unigrams in a file\n",
    "    \"\"\"\n",
    "    text = open(file, \"r\", errors=\"ignore\")\n",
    "    counts = dict()\n",
    "\n",
    "    # Loop through each line of the file\n",
    "    for line in text:\n",
    "        # Split the line into words\n",
    "        words = line.split(\" \")\n",
    "        for unigram in words[1:]:\n",
    "            # skipping first one which is <s>\n",
    "            # Check if the word is already in dictionary\n",
    "            if unigram in counts:\n",
    "                # Increment count of word by 1\n",
    "                counts[unigram] = counts[unigram] + 1\n",
    "            else:\n",
    "                # Add the word to dictionary with count 1\n",
    "                counts[unigram] = 1\n",
    "    return counts\n",
    "            \n",
    "\n",
    "def UnigramMLE(train_counts, test_counts):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of each word\n",
    "    \"\"\"\n",
    "    probs = dict()\n",
    "    vocab_size = len(train_counts)\n",
    "    token_count = sum(train_counts.values())\n",
    "    \n",
    "    for unigram, unigram_count in train_counts.items():\n",
    "        prob_nom = unigram_count\n",
    "        prob_denom = token_count\n",
    "        probs[unigram] = prob_nom / prob_denom\n",
    "        \n",
    "    test_log_likelihood = 0\n",
    "    for n_gram, n_gram_count in test_counts.items():\n",
    "        if n_gram not in probs.keys():\n",
    "            prob = 0.000000000000001\n",
    "        else:\n",
    "            prob = probs[n_gram]\n",
    "        log_likelihood = n_gram_count * log2(prob)\n",
    "        test_log_likelihood += log_likelihood\n",
    "    avg_test_log_likelihood = test_log_likelihood / sum(test_counts.values())\n",
    "    perplexity = 2**(-avg_test_log_likelihood)\n",
    "    return avg_test_log_likelihood, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. A bigram maximum likelihood model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountBigrams(file):\n",
    "    \"\"\"\n",
    "    Count number of bigrams in a file\n",
    "    \"\"\"\n",
    "    text = open(file, \"r\", errors=\"ignore\")\n",
    "    unigram_counts = dict()\n",
    "    bigram_counts = dict()\n",
    "\n",
    "    # Loop through each line of the file\n",
    "    for line in text:\n",
    "        # Split the line into words\n",
    "        words = line.split(\" \")\n",
    "        for unigram in words[1:]:\n",
    "            # skipping first one which is <s>\n",
    "            # Check if the word is already in dictionary\n",
    "            if unigram in unigram_counts:\n",
    "                # Increment count of word by 1\n",
    "                unigram_counts[unigram] = unigram_counts[unigram] + 1\n",
    "            else:\n",
    "                # Add the word to dictionary with count 1\n",
    "                unigram_counts[unigram] = 1\n",
    "                \n",
    "        for i in range(len(words)-1)[1:]:\n",
    "        # skipping first one <s>, therefore adding 1: at the end\n",
    "        # Check if the word is already in dictionary\n",
    "            bigram = (words[i], words[i+1])\n",
    "            if bigram in bigram_counts:\n",
    "                # Increment count of word by 1\n",
    "                bigram_counts[bigram] = bigram_counts[bigram] + 1\n",
    "            else:\n",
    "                # Add the word to dictionary with count 1\n",
    "                bigram_counts[bigram] = 1\n",
    "    \n",
    "    return unigram_counts, bigram_counts\n",
    "            \n",
    "\n",
    "def BigramsMLE(uni_train_counts, bi_train_counts, bi_test_counts):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of each bigram\n",
    "    \"\"\"\n",
    "    probs = dict()\n",
    "    \n",
    "    for bigram, bigram_count in bi_train_counts.items():\n",
    "        word1 = bigram[0]\n",
    "        prob_nom = bigram_count\n",
    "        prob_denom = uni_train_counts.get(word1)\n",
    "        probs[bigram] = prob_nom / prob_denom\n",
    "        \n",
    "    test_log_likelihood = 0\n",
    "    for n_gram, n_gram_count in bi_test_counts.items():\n",
    "        if n_gram not in probs.keys():\n",
    "            prob = 0.000000000000001\n",
    "        else:\n",
    "            prob = probs[n_gram]\n",
    "        log_likelihood = n_gram_count * log2(prob)\n",
    "        test_log_likelihood += log_likelihood\n",
    "    avg_test_log_likelihood = test_log_likelihood / sum(bi_test_counts.values())\n",
    "    perplexity = 2**(-avg_test_log_likelihood)\n",
    "    return avg_test_log_likelihood, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. A bigram model with Add-One smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BigramsMLEAddOneSmooth(uni_train_counts, bi_train_counts, bi_test_counts):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of each bigram\n",
    "    \"\"\"\n",
    "    probs = dict()\n",
    "    vocab_size = len(bi_train_counts)\n",
    "    token_count = sum(bi_train_counts.values())\n",
    "    \n",
    "    for bigram, bigram_count in bi_train_counts.items():\n",
    "        word1 = bigram[0]\n",
    "        prob_nom = bigram_count + 1\n",
    "        prob_denom = uni_train_counts.get(word1) + vocab_size\n",
    "        probs[bigram] = prob_nom / prob_denom\n",
    "    \n",
    "    test_log_likelihood = 0\n",
    "    for n_gram, n_gram_count in bi_test_counts.items():\n",
    "        if n_gram not in probs.keys():\n",
    "            test_word1 = n_gram[0]\n",
    "            prob = 1 / uni_train_counts.get(test_word1) + vocab_size\n",
    "        else:\n",
    "            prob = probs[n_gram]\n",
    "        log_likelihood = n_gram_count * log2(prob)\n",
    "        test_log_likelihood += log_likelihood\n",
    "    avg_test_log_likelihood = test_log_likelihood / sum(bi_test_counts.values())\n",
    "    perplexity = 2**(-avg_test_log_likelihood)\n",
    "    return avg_test_log_likelihood, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. A bigram model with discounting and Katz backoff. Discount constant of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BigramsMLEKatzBackoff(uni_train_counts, bi_train_counts, bi_test_counts, c = 0.5):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of each bigram\n",
    "    \"\"\"\n",
    "    probs = dict()\n",
    "    vocab_size = len(uni_train_counts)\n",
    "    token_count = sum(uni_train_counts.values())\n",
    "    \n",
    "    for bigram, bigram_count in bi_train_counts.items():\n",
    "        word1 = bigram[0]\n",
    "        prob_nom = bigram_count\n",
    "        prob_denom = uni_train_counts.get(word1)\n",
    "        probs[bigram] = prob_nom / prob_denom\n",
    "\n",
    "    test_log_likelihood = 0\n",
    "    for n_gram, n_gram_count in bi_test_counts.items():\n",
    "        if n_gram not in probs.keys():\n",
    "            test_word1 = n_gram[0]\n",
    "            prob = c * (uni_train_counts.get(test_word1) / token_count)\n",
    "        else:\n",
    "            prob = probs[n_gram]\n",
    "        log_likelihood = n_gram_count * log2(prob)\n",
    "        test_log_likelihood += log_likelihood\n",
    "    avg_test_log_likelihood = test_log_likelihood / sum(bi_test_counts.values())\n",
    "    perplexity = 2**(-avg_test_log_likelihood)\n",
    "    return avg_test_log_likelihood, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-10.099596076139623, 1097.188793154414)\n",
      "(-16.005691885679507, 65795.07085373304)\n",
      "(-6.292954575609962, 78.40939242153524)\n",
      "(-8.118504031545793, 277.9157971414787)\n"
     ]
    }
   ],
   "source": [
    "# Unigram Model \n",
    "uni_train_counts = CountUnigrams('train3.txt')\n",
    "uni_test_counts = CountUnigrams('test3.txt')\n",
    "print(UnigramMLE(uni_train_counts, uni_test_counts))\n",
    "\n",
    "#Bigram Model\n",
    "uni_train_counts, bi_train_counts = CountBigrams('train3.txt')\n",
    "uni_test_counts, bi_test_counts = CountBigrams('test3.txt')\n",
    "print(BigramsMLE(uni_train_counts, bi_train_counts, bi_test_counts))\n",
    "\n",
    "#Bigram Model with Add One Smoothening\n",
    "uni_train_counts, bi_train_counts = CountBigrams('train3.txt')\n",
    "uni_test_counts, bi_test_counts = CountBigrams('test3.txt')\n",
    "print(BigramsMLEAddOneSmooth(uni_train_counts, bi_train_counts, bi_test_counts))\n",
    "\n",
    "#Bigram Model with KatzBackoff\n",
    "uni_train_counts, bi_train_counts = CountBigrams('train3.txt')\n",
    "uni_test_counts, bi_test_counts = CountBigrams('test3.txt')\n",
    "print(BigramsMLEKatzBackoff(uni_train_counts, bi_train_counts, bi_test_counts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
