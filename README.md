# LanguageModelsScratch

Functions to compute the perplexity and average log likelihood of test data by training serveral language models on train data. Language models built from scrach. Models explored:

1. A unigram maximum likelihood model.
2. A bigram maximum likelihood model.
3. A bigram model with Add-One smoothing.
4. A bigram model with discounting and Katz backoff. 

Language modeling is crucial in modern NLP applications. It allows computers to understand qualitative information. Used for speech regcognition, chat bots, etc.

Train and test data tokenized provided, for use-case example. 
